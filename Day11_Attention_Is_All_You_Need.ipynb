{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPVy2/tMvHzURKC6FrqT4VK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jane-mystika/AI_regular_tasks/blob/main/Day11_Attention_Is_All_You_Need.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Attention Is All You Need\n",
        "\n",
        "Attention Is All You Need is a research paper published in 2017 by Vaswani et al.\n",
        "The paper introduces the Transformer architecture for sequence modeling.\n",
        "It removes the use of RNNs and CNNs completely.\n",
        "The core idea is that attention alone is sufficient to model sequences.\n",
        "The model uses self-attention to understand relationships between all words in a sentence.\n",
        "Self-attention helps capture long-range dependencies effectively.\n",
        "The Transformer follows an encoderâ€“decoder structure.\n",
        "The encoder processes the input sequence into contextual representations.\n",
        "The decoder generates the output sequence step by step.\n",
        "Multi-head attention allows the model to learn different patterns simultaneously.\n",
        "Scaled dot-product attention is used for efficient computation.\n",
        "Positional encoding is added to maintain word order information.\n",
        "The architecture allows parallel processing, unlike RNNs.\n",
        "This results in faster training and better scalability.\n",
        "The Transformer is the foundation of modern models like BERT, GPT, and T5."
      ],
      "metadata": {
        "id": "QLc1M72MrWBn"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XFJWjH8-qRxB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}